#!/usr/bin/env python
# coding: utf-8

# ¬°Hola!
# 
# Mi nombre es Tonatiuh Cruz. Me complace revisar tu proyecto hoy.
# 
# Al identificar cualquier error inicialmente, simplemente los destacar√©. Te animo a localizar y abordar los problemas de forma independiente como parte de tu preparaci√≥n para un rol como data-scientist. En un entorno profesional, tu l√≠der de equipo seguir√≠a un enfoque similar. Si encuentras la tarea desafiante, proporcionar√© una pista m√°s espec√≠fica en la pr√≥xima iteraci√≥n.
# 
# Encontrar√°s mis comentarios a continuaci√≥n - **por favor no los muevas, modifiques o elimines**.
# 
# Puedes encontrar mis comentarios en cajas verdes, amarillas o rojas como esta:
# 
# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# √âxito. Todo est√° hecho correctamente.
# </div>
# 
# <div class="alert alert-block alert-warning">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Observaciones. Algunas recomendaciones.
# </div>
# 
# <div class="alert alert-block alert-danger">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Necesita correcci√≥n. El bloque requiere algunas correcciones. El trabajo no puede ser aceptado con comentarios en rojo.
# </div>
# 
# Puedes responderme utilizando esto:
# 
# <div class="alert alert-block alert-info">
# <b>Respuesta del estudiante.</b> <a class="tocSkip"></a>
# </div>

# # Descripci√≥n

# La compa√±√≠a de seguros Sure Tomorrow quiere resolver varias tareas con la ayuda de machine learning y te pide que eval√∫es esa posibilidad.
# - Tarea 1: encontrar clientes que sean similares a un cliente determinado. Esto ayudar√° a los agentes de la compa√±√≠a con el marketing.
# - Tarea 2: predecir la probabilidad de que un nuevo cliente reciba una prestaci√≥n del seguro. ¬øPuede un modelo de predictivo funcionar mejor que un modelo dummy?
# - Tarea 3: predecir el n√∫mero de prestaciones de seguro que un nuevo cliente pueda recibir utilizando un modelo de regresi√≥n lineal.
# - Tarea 4: proteger los datos personales de los clientes sin afectar al modelo del ejercicio anterior. Es necesario desarrollar un algoritmo de transformaci√≥n de datos que dificulte la recuperaci√≥n de la informaci√≥n personal si los datos caen en manos equivocadas. Esto se denomina enmascaramiento u ofuscaci√≥n de datos. Pero los datos deben protegerse de tal manera que no se vea afectada la calidad de los modelos de machine learning. No es necesario elegir el mejor modelo, basta con demostrar que el algoritmo funciona correctamente.
# 

# # Preprocesamiento y exploraci√≥n de datos
# 
# ## Inicializaci√≥n

# In[42]:


pip install scikit-learn --upgrade


# In[43]:


import numpy as np
import pandas as pd
import math
import seaborn as sns

import sklearn.linear_model
import sklearn.metrics
import sklearn.neighbors
import sklearn.preprocessing

from sklearn.model_selection import train_test_split

from IPython.display import display
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import f1_score
from sklearn.preprocessing import MaxAbsScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


# ## Carga de datos

# Carga los datos y haz una revisi√≥n b√°sica para comprobar que no hay problemas obvios.

# In[44]:


df = pd.read_csv('/datasets/insurance_us.csv')


# Renombramos las columnas para que el c√≥digo se vea m√°s coherente con su estilo.

# In[45]:


df = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Salary': 'income', 'Family members': 'family_members', 'Insurance benefits': 'insurance_benefits'})


# In[46]:


df.sample(10)


# In[47]:


df.info()


# In[48]:


# puede que queramos cambiar el tipo de edad (de float a int) aunque esto no es crucial

# escribe tu conversi√≥n aqu√≠ si lo deseas:

# Convertir la columna 'age' de float a int
df['age'] = df['age'].astype(int)


# In[49]:


# comprueba que la conversi√≥n se haya realizado con √©xito
print(df.dtypes)


# In[50]:


# ahora echa un vistazo a las estad√≠sticas descriptivas de los datos.# ¬øSe ve todo bien?


# In[51]:


print(df.describe())


# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Muy bien! Importaste de forma correcta las librer√≠as y m√≥dulos, cambiaste el formato de los nombres y verificaste que no hubiera outliers inv√°lidos en los datos.
# </div>

# ## An√°lisis exploratorio de datos

# Vamos a comprobar r√°pidamente si existen determinados grupos de clientes observando el gr√°fico de pares.

# In[52]:


g = sns.pairplot(df, kind='hist')
g.fig.set_size_inches(12, 12)


# De acuerdo, es un poco complicado detectar grupos obvios (cl√∫steres) ya que es dif√≠cil combinar diversas variables simult√°neamente (para analizar distribuciones multivariadas). Ah√≠ es donde LA y ML pueden ser bastante √∫tiles.

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Excelente trabajo con el desarrollo de la gr√°fica de pares. 
# </div>

# # Tarea 1. Clientes similares

# En el lenguaje de ML, es necesario desarrollar un procedimiento que devuelva los k vecinos m√°s cercanos (objetos) para un objeto dado bas√°ndose en la distancia entre los objetos.
# Es posible que quieras revisar las siguientes lecciones (cap√≠tulo -> lecci√≥n)- Distancia entre vectores -> Distancia euclidiana
# - Distancia entre vectores -> Distancia Manhattan
# 
# Para resolver la tarea, podemos probar diferentes m√©tricas de distancia.

# Escribe una funci√≥n que devuelva los k vecinos m√°s cercanos para un $n^{th}$ objeto bas√°ndose en una m√©trica de distancia especificada. A la hora de realizar esta tarea no debe tenerse en cuenta el n√∫mero de prestaciones de seguro recibidas.
# Puedes utilizar una implementaci√≥n ya existente del algoritmo kNN de scikit-learn (consulta [el enlace](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)) o tu propia implementaci√≥n.
# Pru√©balo para cuatro combinaciones de dos casos- Escalado
#   - los datos no est√°n escalados
#   - los datos se escalan con el escalador [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)
# - M√©tricas de distancia
#   - Euclidiana
#   - Manhattan
# 
# Responde a estas preguntas:- ¬øEl hecho de que los datos no est√©n escalados afecta al algoritmo kNN? Si es as√≠, ¬øc√≥mo se manifiesta?- ¬øQu√© tan similares son los resultados al utilizar la m√©trica de distancia Manhattan (independientemente del escalado)?

# In[53]:


feature_names = ['gender', 'age', 'income', 'family_members']


# In[54]:


def get_knn(df, n, k, metric):
    
    """
    Devuelve los k vecinos m√°s cercanos

    :param df: DataFrame de pandas utilizado para encontrar objetos similares dentro del mismo lugar    :param n: n√∫mero de objetos para los que se buscan los vecinos m√°s cercanos    :param k: n√∫mero de vecinos m√°s cercanos a devolver
    :param m√©trica: nombre de la m√©trica de distancia    """

 # Crear el modelo kNN   
    nbrs =NearestNeighbors(n_neighbors=k, metric=metric) # <tu c√≥digo aqu√≠> 
 # Entrenar el modelo
    nbrs.fit(df[feature_names])
 # Obtener las distancias y los √≠ndices de los vecinos m√°s cercanos
    nbrs_distances, nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names]], k, return_distance=True)
 # Preparar el resultado en un DataFrame    
    df_res = pd.concat([
        df.iloc[nbrs_indices[0]], 
        pd.DataFrame(nbrs_distances.T, index=nbrs_indices[0], columns=['distance'])
        ], axis=1)
    
    return df_res


# Escalar datos.

# In[55]:


feature_names = ['gender', 'age', 'income', 'family_members']

transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())

df_scaled = df.copy()
df_scaled.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())


# In[56]:


df_scaled.sample(5)


# Ahora, vamos a obtener registros similares para uno determinado, para cada combinaci√≥n

# In[57]:


# Probar con datos escalados y m√©trica euclidiana
result_scaled_euclidean = get_knn(df_scaled, n=0, k=5, metric="euclidean")
print(result_scaled_euclidean)


# In[58]:


# Probar con datos escalados y m√©trica Manhattan
result_scaled_manhattan = get_knn(df_scaled, n=0, k=5, metric="manhattan")
print(result_scaled_manhattan)


# Respuestas a las preguntas

# **¬øEl hecho de que los datos no est√©n escalados afecta al algoritmo kNN? Si es as√≠, ¬øc√≥mo se manifiesta?** 
# 
# Respuesta: S√≠, afecta. Cuando los datos no est√°n escalados, las caracter√≠sticas con rangos m√°s amplios (como el income) dominan el c√°lculo de las distancias, lo que puede llevar a resultados sesgados. El escalado normaliza las caracter√≠sticas para que todas tengan el mismo peso en el c√°lculo de la distancia.
# 

# **¬øQu√© tan similares son los resultados al utilizar la m√©trica de distancia Manhattan (independientemente del escalado)?** 
# 
# Respuesta: La m√©trica Manhattan tiende a ser menos influenciada por valores extremos y da m√°s importancia a las diferencias absolutas entre las caracter√≠sticas, lo que puede llevar a vecinos ligeramente diferentes en comparaci√≥n con la m√©trica Euclidiana.

# C√≥digo para Validaci√≥n Adicional:
# Vamos a implementar este paso:

# In[59]:


# Probar con diferentes clientes
print("Vecinos m√°s cercanos para el cliente 10 con m√©trica Euclidiana:")
result_scaled_euclidean_10 = get_knn(df_scaled, n=10, k=5, metric="euclidean")
print(result_scaled_euclidean_10)

print("\nVecinos m√°s cercanos para el cliente 20 con m√©trica Euclidiana:")
result_scaled_euclidean_20 = get_knn(df_scaled, n=20, k=5, metric="euclidean")
print(result_scaled_euclidean_20)

# Probar con diferentes valores de k
print("\nVecinos m√°s cercanos para el cliente 0 con m√©trica Euclidiana, k=3:")
result_scaled_euclidean_k3 = get_knn(df_scaled, n=0, k=3, metric="euclidean")
print(result_scaled_euclidean_k3)

print("\nVecinos m√°s cercanos para el cliente 0 con m√©trica Euclidiana, k=10:")
result_scaled_euclidean_k10 = get_knn(df_scaled, n=0, k=10, metric="euclidean")
print(result_scaled_euclidean_k10)


# Conclusi√≥n de la Validaci√≥n:
# Consistencia: La funci√≥n get_knn es consistente y devuelve vecinos similares al cliente objetivo, lo que confirma que est√° funcionando correctamente.
# Variaci√≥n con k: Al variar k, la funci√≥n sigue siendo robusta, aunque se observa que, al aumentar k, la variabilidad en las caracter√≠sticas de los vecinos tambi√©n aumenta, lo cual es normal.

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Bien implementado el modelo. Como acabas de notar, el escalado es muy importante debido a que permite que variables num√©ricas con rangos muy diferentes sean comparables. Sin escalar los datos las caracter√≠sticas con valores m√°s grandes pueden dominar las m√©tricas de distancia y no obtener resultados precisos.
# </div>

# # Tarea 2. ¬øEs probable que el cliente reciba una prestaci√≥n del seguro?

# En t√©rminos de machine learning podemos considerarlo como una tarea de clasificaci√≥n binaria.

# Con el valor de `insurance_benefits` superior a cero como objetivo, eval√∫a si el enfoque de clasificaci√≥n kNN puede funcionar mejor que el modelo dummy.
# Instrucciones:
# - Construye un clasificador basado en KNN y mide su calidad con la m√©trica F1 para k=1...10 tanto para los datos originales como para los escalados. Ser√≠a interesante observar c√≥mo k puede influir en la m√©trica de evaluaci√≥n y si el escalado de los datos provoca alguna diferencia. Puedes utilizar una implementaci√≥n ya existente del algoritmo de clasificaci√≥n kNN de scikit-learn (consulta [el enlace](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) o tu propia implementaci√≥n.- Construye un modelo dummy que, en este caso, es simplemente un modelo aleatorio. Deber√≠a devolver "1" con cierta probabilidad. Probemos el modelo con cuatro valores de probabilidad: 0, la probabilidad de pagar cualquier prestaci√≥n del seguro, 0.5, 1.
# La probabilidad de pagar cualquier prestaci√≥n del seguro puede definirse como
# $$
# P\{\text{prestaci√≥n de seguro recibida}\}=\frac{\text{n√∫mero de clientes que han recibido alguna prestaci√≥n de seguro}}{\text{n√∫mero total de clientes}}.
# $$
# 
# Divide todos los datos correspondientes a las etapas de entrenamiento/prueba respetando la proporci√≥n 70:30.

# Paso 1: Definir la Variable Objetivo

# In[60]:


# —Åalcula el objetivo , definir la variable objetivo
df['insurance_benefits_received'] = (df['insurance_benefits'] > 0).astype(int) #<tu c√≥digo aqu√≠>


# In[61]:


# comprueba el desequilibrio de clases con value_counts()

print(df['insurance_benefits_received'].value_counts(normalize=True))


# Paso 2: Dividir los Datos

# In[62]:


# Definir las caracter√≠sticas y la variable objetivo
X = df[['gender', 'age', 'income', 'family_members']]  # Caracter√≠sticas
y = df['insurance_benefits_received']  # Variable objetivo

# Dividir los datos en entrenamiento y prueba (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# Paso 3: Evaluar el Clasificador kNN
# Aqu√≠ es donde entrenas el modelo kNN y eval√∫as su rendimiento. Utilizar√°s el c√≥digo proporcionado para construir un clasificador kNN y medir la m√©trica F1 para diferentes valores de k

# In[63]:


def eval_classifier(y_true, y_pred):
    
    f1_score = sklearn.metrics.f1_score(y_true, y_pred)
    print(f'F1: {f1_score:.2f}')
    
# si tienes alg√∫n problema con la siguiente l√≠nea, reinicia el kernel y ejecuta el cuaderno de nuevo    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')
    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')
    print('Matriz de confusi√≥n')
    print(cm)
    


# In[64]:


# Entrenar y evaluar el clasificador kNN
for k in range(1, 11):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    print(f'k={k}')
    eval_classifier(y_test, y_pred)


# Paso 4: Construir el Modelo Dummy
# Finalmente, construimos el modelo dummy que predice de manera aleatoria si un cliente recibir√° beneficios. Este modelo sirve como referencia para comparar el rendimiento del kNN.

# In[65]:


# generar la salida de un modelo aleatorio

def rnd_model_predict(P, size, seed=42):

    rng = np.random.default_rng(seed=seed)
    return rng.binomial(n=1, p=P, size=size)


# In[66]:


# Calcular la probabilidad observada de recibir una prestaci√≥n
P_observed = df['insurance_benefits_received'].sum() / len(df)


# In[67]:


# Evaluar el modelo dummy con diferentes probabilidades
for P in [0, P_observed, 0.5, 1]:
    print(f'Probabilidad: {P:.2f}')
    y_pred_rnd = rnd_model_predict(P, size=len(y_test))
    eval_classifier(y_test, y_pred_rnd)
    print()


# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Excelente! Tu balanceo de clases usando downsampling, separaci√≥n de datos y resultados del modelo fueron muy buenos! Cu√°l ser√≠a tu intepretaci√≥n del hecho que el escalado de datos mejore considerablemente el F1?
#     
# </div>

# # Tarea 3. Regresi√≥n (con regresi√≥n lineal)

# Con `insurance_benefits` como objetivo, eval√∫a cu√°l ser√≠a la RECM de un modelo de regresi√≥n lineal.

# Construye tu propia implementaci√≥n de regresi√≥n lineal. Para ello, recuerda c√≥mo est√° formulada la soluci√≥n de la tarea de regresi√≥n lineal en t√©rminos de LA. Comprueba la RECM tanto para los datos originales como para los escalados. ¬øPuedes ver alguna diferencia en la RECM con respecto a estos dos casos?
# 
# Denotemos- $X$: matriz de caracter√≠sticas; cada fila es un caso, cada columna es una caracter√≠stica, la primera columna est√° formada por unidades- $y$ ‚Äî objetivo (un vector)- $\hat{y}$ ‚Äî objetivo estimado (un vector)- $w$ ‚Äî vector de pesos
# La tarea de regresi√≥n lineal en el lenguaje de las matrices puede formularse as√≠:
# $$
# y = Xw
# $$
# 
# El objetivo de entrenamiento es entonces encontrar esa $w$ w que minimice la distancia L2 (ECM) entre $Xw$ y $y$:
# 
# $$
# \min_w d_2(Xw, y) \quad \text{or} \quad \min_w \text{MSE}(Xw, y)
# $$
# 
# Parece que hay una soluci√≥n anal√≠tica para lo anteriormente expuesto:
# $$
# w = (X^T X)^{-1} X^T y
# $$
# 
# La f√≥rmula anterior puede servir para encontrar los pesos $w$ y estos √∫ltimos pueden utilizarse para calcular los valores predichos
# $$
# \hat{y} = X_{val}w
# $$

# Divide todos los datos correspondientes a las etapas de entrenamiento/prueba respetando la proporci√≥n 70:30. Utiliza la m√©trica RECM para evaluar el modelo.

# Paso 1: Definir la Clase MyLinearRegression
# El pre-c√≥digo sugiere construir una implementaci√≥n personalizada de un modelo de regresi√≥n lineal. Este modelo se basar√° en la f√≥rmula matem√°tica para obtener los pesos ùë§ y realizar predicciones.

# In[68]:


class MyLinearRegression:
    
    def __init__(self):
        
        self.weights = None    # Inicializa los pesos como None
    
    def fit(self, X, y):
         # A√±adir una columna de unos a X para representar el t√©rmino independiente (bias)
        # a√±adir las unidades
        X2 = np.append(np.ones([len(X), 1]), X, axis=1)
         # Calcular los pesos utilizando la f√≥rmula: w = (X^T X)^-1 X^T y
        self.weights = np.linalg.inv(X2.T @ X2) @ X2.T @ y

    def predict(self, X):
        # A√±adir una columna de unos a X para representar el t√©rmino independiente (bias)
        # a√±adir las unidades
        X2 = np.append(np.ones([len(X), 1]), X, axis=1)
        # Realizar la predicci√≥n utilizando los pesos calculados
        y_pred =  X2 @ self.weights
        
        return y_pred


# Paso 2: Definir la Funci√≥n de Evaluaci√≥n eval_regressor
# Vamos a definir la funci√≥n que evaluar√° el rendimiento del modelo utilizando la m√©trica RECM (RMSE) y el coeficiente de determinaci√≥n R2.

# In[69]:


def eval_regressor(y_true, y_pred):
    # Calcular el RMSE
    rmse = math.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))
    print(f'RMSE: {rmse:.2f}')
    # Calcular el R2
    r2_score = math.sqrt(sklearn.metrics.r2_score(y_true, y_pred))
    print(f'R2: {r2_score:.2f}')    


# Paso 3: Entrenar y Evaluar el Modelo de Regresi√≥n Lineal
# Ahora, entrenaremos el modelo de regresi√≥n lineal personalizado (MyLinearRegression) utilizando los datos de entrenamiento, y luego evaluaremos su rendimiento en los datos de prueba.

# In[70]:


# Paso 3: Preparar los datos y entrenar el modelo
X = df[['age', 'gender', 'income', 'family_members']].to_numpy()
y = df['insurance_benefits'].to_numpy()

# Dividir los datos en entrenamiento y prueba (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)

# Crear una instancia del modelo
lr = MyLinearRegression()

# Entrenar el modelo en los datos de entrenamiento
lr.fit(X_train, y_train)
print(lr.weights)

# Realizar predicciones en los datos de prueba
y_test_pred = lr.predict(X_test)
# Evaluar el modelo en los datos de prueba
eval_regressor(y_test, y_test_pred)


# Paso 4: Evaluar el Modelo con Datos Escalados
# Tambi√©n es √∫til escalar los datos para ver si esto afecta la precisi√≥n del modelo. Vamos a utilizar el MaxAbsScaler como en las tareas anteriores y repetir el proceso.

# In[71]:


# Escalar los datos
scaler = MaxAbsScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Crear una instancia del modelo
lr_scaled = MyLinearRegression()

# Entrenar el modelo en los datos de entrenamiento escalados
lr_scaled.fit(X_train_scaled, y_train)

# Realizar predicciones en los datos de prueba escalados
y_test_pred_scaled = lr_scaled.predict(X_test_scaled)

# Evaluar el modelo en los datos de prueba escalados
eval_regressor(y_test, y_test_pred_scaled)


# El modelo de regresi√≥n lineal personalizado ha demostrado ser eficaz para predecir el n√∫mero de prestaciones de seguro que un nuevo cliente podr√≠a recibir, con un buen nivel de precisi√≥n (RMSE bajo y R2 aceptable). Adem√°s, el hecho de que los resultados sean consistentes tanto para los datos originales como para los escalados sugiere que el modelo es robusto y bien ajustado.

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Muy bien! Como pudiste notar usar los datos originales o los escalados no afectan los resultados del modelo de regresi√≥n lineal debido a que la relaci√≥n lineal entre las variables no cambia, simplemente se reescalan. 
#     
# </div>

# # Tarea 4. Ofuscar datos

# Lo mejor es ofuscar los datos multiplicando las caracter√≠sticas num√©ricas (recuerda que se pueden ver como la matriz $X$) por una matriz invertible $P$. 
# 
# $$
# X' = X \times P
# $$
# 
# Trata de hacerlo y comprueba c√≥mo quedar√°n los valores de las caracter√≠sticas despu√©s de la transformaci√≥n. Por cierto, la propiedad de invertibilidad es importante aqu√≠, as√≠ que aseg√∫rate de que $P$ sea realmente invertible.
# 
# Puedes revisar la lecci√≥n 'Matrices y operaciones matriciales -> Multiplicaci√≥n de matrices' para recordar la regla de multiplicaci√≥n de matrices y su implementaci√≥n con NumPy.

# Paso 1: Ofuscar los Datos con una Matriz Invertible
# Primero, necesitamos crear una matriz aleatoria ùëÉ y comprobar que sea invertible. Luego, multiplicaremos la matriz 
# ùëã (caracter√≠sticas num√©ricas de los datos) por ùëÉ para obtener los datos ofuscados X.
# 

# In[72]:


# Seleccionar las columnas con informaci√≥n personal
personal_info_column_list = ['gender', 'age', 'income', 'family_members']
df_pn = df[personal_info_column_list]


# In[73]:


X = df_pn.to_numpy()


# Generar una matriz aleatoria $P$.

# In[74]:


rng = np.random.default_rng(seed=42)
P = rng.random(size=(X.shape[1], X.shape[1]))


# Comprobar que la matriz P sea invertible

# In[75]:


if np.linalg.det(P) != 0:
    print("La matriz P es invertible.")
else:
    raise ValueError("La matriz P no es invertible. Intente generar otra matriz.")

# Ofuscar los datos
X_obfuscated = X @ P

# Mostrar algunos ejemplos de los datos originales y ofuscados
print("Datos originales:\n", X[:5])
print("Datos ofuscados:\n", X_obfuscated[:5])


# ¬øPuedes adivinar la edad o los ingresos de los clientes despu√©s de la transformaci√≥n?

# Aqu√≠, el objetivo es demostrar que si conocemos la matriz de transformaci√≥n ùëÉ, podemos recuperar los datos originales aplicando la operaci√≥n inversa. Esto se puede hacer invirtiendo la matriz ùëÉ y multiplicando los datos transformados por la inversa de ùëÉ.

# ¬øPuedes recuperar los datos originales de $X'$ si conoces $P$? Intenta comprobarlo a trav√©s de los c√°lculos moviendo $P$ del lado derecho de la f√≥rmula anterior al izquierdo. En este caso las reglas de la multiplicaci√≥n matricial son realmente √∫tiles

# Paso 2: Intentar Recuperar los Datos Originales
# Para comprobar que la transformaci√≥n es reversible, vamos a intentar recuperar los datos originales utilizando la matriz 
# ùëÉ‚àí1

# In[76]:


# Recuperar los datos originales
P_inv = np.linalg.inv(P)
X_recovered = X_obfuscated @ P_inv


# Muestra los tres casos para algunos clientes- Datos originales
# - El que est√° transformado- El que est√° invertido (recuperado)

# In[77]:


# Mostrar algunos ejemplos de los datos recuperados
print("\nDatos recuperados:\n", X_recovered[:5])

# Comparar los datos recuperados con los originales
print("\nDatos originales:\n", X[:5])  # Muestra los primeros 5 registros originales

print("\nDatos transformados:\n", X_obfuscated[:5])  # Muestra los primeros 5 registros transformados

print("\nDatos recuperados:\n", X_recovered[:5])  # Muestra los primeros 5 registros recuperados


# Seguramente puedes ver que algunos valores no son exactamente iguales a los de los datos originales. ¬øCu√°l podr√≠a ser la raz√≥n de ello?

# En la pr√°ctica, la inversi√≥n de matrices y la multiplicaci√≥n de matrices pueden introducir peque√±os errores debido a la naturaleza de los c√°lculos en coma flotante. Esto significa que los valores recuperados podr√≠an no ser exactamente iguales a los originales, aunque deber√≠an ser muy cercanos.
# 
# Si los valores recuperados no son exactamente iguales, esto se debe a la acumulaci√≥n de errores num√©ricos en la operaci√≥n de inversi√≥n y multiplicaci√≥n de matrices.

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Excelente! Lograste ofuscar los datos originales de forma adecuada y despu√©s recuperarlos a trav√©s de operaciones matriciales. Si bien hay una diferencia muy peque√±a entre los datos orginales y los datos recuperados, se debe a los redondeos que se est√°n realizando al hacer las operaciones.
# </div>

# ## Prueba de que la ofuscaci√≥n de datos puede funcionar con regresi√≥n lineal

# En este proyecto la tarea de regresi√≥n se ha resuelto con la regresi√≥n lineal. Tu siguiente tarea es demostrar _analytically_ que el m√©todo de ofuscaci√≥n no afectar√° a la regresi√≥n lineal en t√©rminos de valores predichos, es decir, que sus valores seguir√°n siendo los mismos. ¬øLo puedes creer? Pues no hace falta que lo creas, ¬°tienes que que demostrarlo!

# Entonces, los datos est√°n ofuscados y ahora tenemos $X \times P$ en lugar de tener solo $X$. En consecuencia, hay otros pesos $w_P$ como
# $$
# w = (X^T X)^{-1} X^T y \quad \Rightarrow \quad w_P = [(XP)^T XP]^{-1} (XP)^T y
# $$
# 
# ¬øC√≥mo se relacionar√≠an $w$ y $w_P$ si simplific√°ramos la f√≥rmula de $w_P$ anterior? 
# 
# ¬øCu√°les ser√≠an los valores predichos con $w_P$? 
# 
# ¬øQu√© significa esto para la calidad de la regresi√≥n lineal si esta se mide mediante la RECM?
# Revisa el Ap√©ndice B Propiedades de las matrices al final del cuaderno. ¬°All√≠ encontrar√°s f√≥rmulas muy √∫tiles!
# 
# No es necesario escribir c√≥digo en esta secci√≥n, basta con una explicaci√≥n anal√≠tica.

# **Respuesta**

# ¬øC√≥mo se relacionar√≠an  $ùë§$ y  $ùë§_ùëÉ$ si simplific√°ramos la f√≥rmula de  $ùë§_ùëÉ$ anterior?
# 
# Los nuevos pesos $w_P$ est√°n relacionados con los pesos originales $w$ a trav√©s de la inversa de la matriz de transformaci√≥n ùëÉ.
# 
# En concreto:  $W_P$=$P^{‚àí1}$ $w$
# 
# ¬øCu√°les ser√≠an los valores predichos con  $ùë§_ùëÉ$?
# 
# Los valores predichos $\hat{y}p$ utilizando los datos ofuscados ser√°n exactamente los mismos que los valores predichos utilizando los datos originales $\hat{y}$ Esto demuestra que la ofuscaci√≥n de datos no afecta la calidad de las predicciones en t√©rminos de la regresi√≥n lineal.
# $\hat{y}p$= $XI_w$ = Xw = $\hat{y}$
#  
# ¬øQu√© significa esto para la calidad de la regresi√≥n lineal si esta se mide mediante la RECM? Revisa el Ap√©ndice B Propiedades de las matrices al final del cuaderno. ¬°All√≠ encontrar√°s f√≥rmulas muy √∫tiles!
# 
# Dado que los valores predichos $\hat{y}p$ son id√©nticos a los valores predichos $\hat{y}$ cuando utilizamos la ofuscaci√≥n, las m√©tricas de evaluaci√≥n como el RMSE (Root Mean Squared Error) y el $ùëÖ^2$ no se ver√°n afectadas. Por lo tanto, la calidad del modelo de regresi√≥n lineal permanecer√° inalterada.

# **Prueba anal√≠tica**

# Pregunta 1: ¬øC√≥mo se relacionar√≠an los pesos $ùë§$ y $ùë§ùëÉ$ si simplific√°ramos la f√≥rmula de $ùë§ùëÉ$?
# 
# *Partimos de la f√≥rmula general de los pesos en la regresi√≥n lineal:
# 
# w = ($X^T$ X)$^{-1}$ $X^T$y
# 
# *Con la matriz de caracter√≠sticas ofuscada $X_p$ = X x P los nuevos pesos $W_p$ se calculan como:
# 
# $w_P$ = [$(X_P)^T$ $X_P$]$^{-1}$ $(X_P)^T$ $y$  = [$P^T$ $X^T$ $XP$]$^{-1}$ $P^T$ $X^T$ $y$
# 
# *Aplicando las propiedades de las matrices, podemos simplificar esto:
# 
# $w_P$ = $P^{-1}$ ($X^T$ $X$)$^{-1}$ $X^T$ $y$ = $P^{-1}$ $w$
# 
# Conclusi√≥n: Los nuevos pesos $w_p$ est√°n relacionados con los pesos originales $w$ a trav√©s de la matriz inversa de la transformaci√≥n $ùëÉ$
# 
# Pregunta 2: ¬øCu√°les ser√≠an los valores predichos con $w_p$ ?
# 
# *Los valores predichos en la regresi√≥n lineal se obtienen como:
# 
# $\hat{y}$ = Xw
# 
# *Para los datos ofuscados, se tiene:
# 
# $\hat{y}p$= $X_p$$w_p$= ( X x P ) ($P^{-1}$w)
# 
# *Simplificando:
# 
# $\hat{y}p$= $XI_w$ = Xw = $\hat{y}$
# 
# *Conclusi√≥n: Los valores predichos con $w_p$ ser√°n id√©nticos a los valores predichos con ùë§
# 
# Pregunta 3 : La ofuscaci√≥n de datos mediante la multiplicaci√≥n por una matriz invertible $ùëÉ$ no afecta la calidad de la regresi√≥n lineal, ni en t√©rminos de los valores predichos ni en t√©rminos de las m√©tricas de evaluaci√≥n como RMSE o $R^2$
# 

# <div class="alert alert-block alert-danger">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Excelente! Solamente verifica la manera en que muestras el apartado dado que parece existir un error en el c√≥digo
# </div>

# <div class="alert alert-block alert-info">
# <b>Respuesta del estudiante.</b> <a class="tocSkip"></a>
# </div>
# 
# No entendi a que se refiere que hay error de codigo , no entiendo el comentario para corregirlo , por favor entregarme mas detalles al respecto porque es la escritura de algebra de formulas lo que esta aqui y no codigo , entonces ahi me perdi. Favor mas detalles para poder corregir la informacion. Saludos 

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Hola, ya se puede ver correctamente el c√≥digo. Puede ser un problema de la plataforma y en la iteraci√≥n pasada me marcada error
# </div>

# ## Prueba de regresi√≥n lineal con ofuscaci√≥n de datos

# Ahora, probemos que la regresi√≥n lineal pueda funcionar, en t√©rminos computacionales, con la transformaci√≥n de ofuscaci√≥n elegida.
# Construye un procedimiento o una clase que ejecute la regresi√≥n lineal opcionalmente con la ofuscaci√≥n. Puedes usar una implementaci√≥n de regresi√≥n lineal de scikit-learn o tu propia implementaci√≥n.
# Ejecuta la regresi√≥n lineal para los datos originales y los ofuscados, compara los valores predichos y los valores de las m√©tricas RMSE y $R^2$. ¬øHay alguna diferencia?

# **Procedimiento**
# 
# - Crea una matriz cuadrada $P$ de n√∫meros aleatorios.- Comprueba que sea invertible. Si no lo es, repite el primer paso hasta obtener una matriz invertible.- <¬° tu comentario aqu√≠ !>
# - Utiliza $XP$ como la nueva matriz de caracter√≠sticas

# Paso 1: Crear una matriz cuadrada ùëÉ de n√∫meros aleatorios e invertirla.

# In[78]:


# Ya se gener√≥ anteriormente una matriz P y se comprob√≥ que es invertible.
# Continuamos utilizando esa misma matriz P y P_inv.


# Paso 2: Utilizar ùëã√óùëÉ como la nueva matriz de caracter√≠sticas y comparar los resultados.

# In[79]:


# Crear el modelo de regresi√≥n lineal utilizando los datos originales
model_original = LinearRegression()
model_original.fit(X_train, y_train)
y_pred_original = model_original.predict(X_test)

# RMSE y R2 para los datos originales
rmse_original = mean_squared_error(y_test, y_pred_original, squared=False)
r2_original = r2_score(y_test, y_pred_original)

# Crear el modelo de regresi√≥n lineal utilizando los datos ofuscados
X_train_obfuscated = X_train @ P
X_test_obfuscated = X_test @ P

model_obfuscated = LinearRegression()
model_obfuscated.fit(X_train_obfuscated, y_train)
y_pred_obfuscated = model_obfuscated.predict(X_test_obfuscated)

# RMSE y R2 para los datos ofuscados
rmse_obfuscated = mean_squared_error(y_test, y_pred_obfuscated, squared=False)
r2_obfuscated = r2_score(y_test, y_pred_obfuscated)

# Mostrar los resultados
print(f"RMSE (original): {rmse_original:.4f}")
print(f"R2 (original): {r2_original:.4f}")
print(f"RMSE (ofuscado): {rmse_obfuscated:.4f}")
print(f"R2 (ofuscado): {r2_obfuscated:.4f}")


# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Como ya demostraste anteriormente no existe diferencias entre el RMSE y R2 usando los datos originales y los datos ofuscados.
# </div>

# # Conclusiones

# 1. Igualdad de Resultados Entre Datos Originales y Ofuscados:
# RMSE (Ra√≠z del Error Cuadr√°tico Medio): Tanto para los datos originales como para los datos ofuscados, la RMSE es exactamente la misma (0.3436). Esto significa que la precisi√≥n del modelo, en t√©rminos de error medio, no se ve afectada por la ofuscaci√≥n de los datos.
# R¬≤ (Coeficiente de Determinaci√≥n): Al igual que la RMSE, el R¬≤ es id√©ntico para ambos conjuntos de datos (0.4305). Esto indica que la capacidad del modelo para explicar la varianza en los datos objetivo es la misma, independientemente de si los datos han sido ofuscados o no.

# 2. Impacto de la Ofuscaci√≥n en la Regresi√≥n Lineal:
# La ofuscaci√≥n de datos mediante la multiplicaci√≥n de las caracter√≠sticas por una matriz invertible no altera los resultados de la regresi√≥n lineal. Los valores predichos y las m√©tricas de evaluaci√≥n (RMSE y R¬≤) permanecen inalterados.
# Esto se debe a que la multiplicaci√≥n por una matriz invertible y su inversa es una transformaci√≥n lineal que preserva la estructura de los datos en t√©rminos de relaciones lineales. La regresi√≥n lineal, siendo una t√©cnica que busca relaciones lineales entre las caracter√≠sticas y el objetivo, es robusta frente a este tipo de transformaciones.

# 3. Implicaciones Pr√°cticas:
# La ofuscaci√≥n de datos es una t√©cnica viable para proteger la informaci√≥n sensible sin comprometer la precisi√≥n y eficacia de los modelos de machine learning, espec√≠ficamente en el contexto de la regresi√≥n lineal.
# Esta propiedad es especialmente valiosa en aplicaciones donde la privacidad y la seguridad de los datos son cr√≠ticas, permitiendo que se mantenga la funcionalidad del modelo sin exponer informaci√≥n confidencial.
# En resumen, la prueba demuestra que la ofuscaci√≥n de datos, cuando se realiza correctamente, no afecta negativamente el rendimiento de la regresi√≥n lineal, lo que la convierte en una t√©cnica efectiva para proteger los datos en contextos donde la privacidad es fundamental.

# # Lista de control

# Escribe 'x' para verificar. Luego presiona Shift+Enter.

# - [x]  Jupyter Notebook est√° abierto
# - [ ]  El c√≥digo no tiene errores- [ ]  Las celdas est√°n ordenadas de acuerdo con la l√≥gica y el orden de ejecuci√≥n
# - [ ]  Se ha realizado la tarea 1
#     - [ ]  Est√° presente el procedimiento que puede devolver k clientes similares para un cliente determinado
#     - [ ]  Se prob√≥ el procedimiento para las cuatro combinaciones propuestas    - [ ]  Se respondieron las preguntas sobre la escala/distancia- [ ]  Se ha realizado la tarea 2
#     - [ ]  Se construy√≥ y prob√≥ el modelo de clasificaci√≥n aleatoria para todos los niveles de probabilidad    - [ ]  Se construy√≥ y prob√≥ el modelo de clasificaci√≥n kNN tanto para los datos originales como para los escalados. Se calcul√≥ la m√©trica F1.- [ ]  Se ha realizado la tarea 3
#     - [ ]  Se implement√≥ la soluci√≥n de regresi√≥n lineal mediante operaciones matriciales    - [ ]  Se calcul√≥ la RECM para la soluci√≥n implementada- [ ]  Se ha realizado la tarea 4
#     - [ ]  Se ofuscaron los datos mediante una matriz aleatoria e invertible P    - [ ]  Se recuperaron los datos ofuscados y se han mostrado algunos ejemplos    - [ ]  Se proporcion√≥ la prueba anal√≠tica de que la transformaci√≥n no afecta a la RECM    - [ ]  Se proporcion√≥ la prueba computacional de que la transformaci√≥n no afecta a la RECM- [ ]  Se han sacado conclusiones

# # Ap√©ndices
# 
# ## Ap√©ndice A: Escribir f√≥rmulas en los cuadernos de Jupyter

# Puedes escribir f√≥rmulas en tu Jupyter Notebook utilizando un lenguaje de marcado proporcionado por un sistema de publicaci√≥n de alta calidad llamado $\LaTeX$ (se pronuncia como "Lah-tech"). Las f√≥rmulas se ver√°n como las de los libros de texto.
# 
# Para incorporar una f√≥rmula a un texto, pon el signo de d√≥lar (\\$) antes y despu√©s del texto de la f√≥rmula, por ejemplo: $\frac{1}{2} \times \frac{3}{2} = \frac{3}{4}$ or $y = x^2, x \ge 1$.
# 
# Si una f√≥rmula debe estar en el mismo p√°rrafo, pon el doble signo de d√≥lar (\\$\\$) antes y despu√©s del texto de la f√≥rmula, por ejemplo:
# $$
# \bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i.
# $$
# 
# El lenguaje de marcado de [LaTeX](https://es.wikipedia.org/wiki/LaTeX) es muy popular entre las personas que utilizan f√≥rmulas en sus art√≠culos, libros y textos. Puede resultar complicado, pero sus fundamentos son sencillos. Consulta esta [ficha de ayuda](http://tug.ctan.org/info/undergradmath/undergradmath.pdf) (materiales en ingl√©s) de dos p√°ginas para aprender a componer las f√≥rmulas m√°s comunes.

# ## Ap√©ndice B: Propiedades de las matrices

# Las matrices tienen muchas propiedades en cuanto al √°lgebra lineal. Aqu√≠ se enumeran algunas de ellas que pueden ayudarte a la hora de realizar la prueba anal√≠tica de este proyecto.

# <table>
# <tr>
# <td>Distributividad</td><td>$A(B+C)=AB+AC$</td>
# </tr>
# <tr>
# <td>No conmutatividad</td><td>$AB \neq BA$</td>
# </tr>
# <tr>
# <td>Propiedad asociativa de la multiplicaci√≥n</td><td>$(AB)C = A(BC)$</td>
# </tr>
# <tr>
# <td>Propiedad de identidad multiplicativa</td><td>$IA = AI = A$</td>
# </tr>
# <tr>
# <td></td><td>$A^{-1}A = AA^{-1} = I$
# </td>
# </tr>    
# <tr>
# <td></td><td>$(AB)^{-1} = B^{-1}A^{-1}$</td>
# </tr>    
# <tr>
# <td>Reversibilidad de la transposici√≥n de un producto de matrices,</td><td>$(AB)^T = B^TA^T$</td>
# </tr>    
# </table>

# <div class="alert alert-block alert-success">
# <b>Comentario del revisor</b> <a class="tocSkip"></a>
# 
# Gran trabajo! Que sigas disfrutando los siguientes cursos!
# </div>
