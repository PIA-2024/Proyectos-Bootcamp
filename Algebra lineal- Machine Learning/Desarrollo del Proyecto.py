# Preprocesamiento y exploraciÃ³n de datos

import numpy as np
import pandas as pd
import math
import seaborn as sns

import sklearn.linear_model
import sklearn.metrics
import sklearn.neighbors
import sklearn.preprocessing

from sklearn.model_selection import train_test_split

from IPython.display import display
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import f1_score
from sklearn.preprocessing import MaxAbsScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


#Carga de datos

# Carga los datos y haz una revisiÃ³n bÃ¡sica para comprobar que no hay problemas obvios.

df = pd.read_csv('C:/Users/piaal/Documents/PIA/DATA SCIENCE/PROYECTOS BOOTCAMP/Algebra lineal- Machine Learning/insurance_us.csv')


# Renombramos las columnas para que el cÃ³digo se vea mÃ¡s coherente con su estilo.

df = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Salary': 'income', 'Family members': 'family_members', 'Insurance benefits': 'insurance_benefits'})

df.sample(10)
df.info()

# puede que queramos cambiar el tipo de edad (de float a int) aunque esto no es crucial

# Convertir la columna 'age' de float a int
df['age'] = df['age'].astype(int)

# comprueba que la conversiÃ³n se haya realizado con Ã©xito
print(df.dtypes)

print(df.describe())

# AnÃ¡lisis exploratorio de datos

g = sns.pairplot(df, kind='hist')
g.fig.set_size_inches(12, 12)


# De acuerdo, es un poco complicado detectar grupos obvios (clÃºsteres) ya que es difÃ­cil combinar diversas variables simultÃ¡neamente (para analizar distribuciones multivariadas). AhÃ­ es donde LA y ML pueden ser bastante Ãºtiles.

# # Tarea 1. Clientes similares

# En el lenguaje de ML, es necesario desarrollar un procedimiento que devuelva los k vecinos mÃ¡s cercanos (objetos) para un objeto dado basÃ¡ndose en la distancia entre los objetos.
# Es posible que quieras revisar las siguientes lecciones (capÃ­tulo -> lecciÃ³n)- Distancia entre vectores -> Distancia euclidiana
# - Distancia entre vectores -> Distancia Manhattan
# 
# Para resolver la tarea, podemos probar diferentes mÃ©tricas de distancia.

# Escribe una funciÃ³n que devuelva los k vecinos mÃ¡s cercanos para un $n^{th}$ objeto basÃ¡ndose en una mÃ©trica de distancia especificada. A la hora de realizar esta tarea no debe tenerse en cuenta el nÃºmero de prestaciones de seguro recibidas.
# Puedes utilizar una implementaciÃ³n ya existente del algoritmo kNN de scikit-learn (consulta [el enlace](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)) o tu propia implementaciÃ³n.
# PruÃ©balo para cuatro combinaciones de dos casos- Escalado
#   - los datos no estÃ¡n escalados
#   - los datos se escalan con el escalador [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)
# - MÃ©tricas de distancia
#   - Euclidiana
#   - Manhattan
# 
# Responde a estas preguntas:- Â¿El hecho de que los datos no estÃ©n escalados afecta al algoritmo kNN? Si es asÃ­, Â¿cÃ³mo se manifiesta?- Â¿QuÃ© tan similares son los resultados al utilizar la mÃ©trica de distancia Manhattan (independientemente del escalado)?

feature_names = ['gender', 'age', 'income', 'family_members']

def get_knn(df, n, k, metric):
    
    """
    Devuelve los k vecinos mÃ¡s cercanos

    :param df: DataFrame de pandas utilizado para encontrar objetos similares dentro del mismo lugar    :param n: nÃºmero de objetos para los que se buscan los vecinos mÃ¡s cercanos    :param k: nÃºmero de vecinos mÃ¡s cercanos a devolver
    :param mÃ©trica: nombre de la mÃ©trica de distancia    """

 # Crear el modelo kNN   
    nbrs =NearestNeighbors(n_neighbors=k, metric=metric) # <tu cÃ³digo aquÃ­> 
 # Entrenar el modelo
    nbrs.fit(df[feature_names])
 # Obtener las distancias y los Ã­ndices de los vecinos mÃ¡s cercanos
    nbrs_distances, nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names]], k, return_distance=True)
 # Preparar el resultado en un DataFrame    
    df_res = pd.concat([
        df.iloc[nbrs_indices[0]], 
        pd.DataFrame(nbrs_distances.T, index=nbrs_indices[0], columns=['distance'])
        ], axis=1)
    
    return df_res


# Escalar datos.

feature_names = ['gender', 'age', 'income', 'family_members']

transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())

df_scaled = df.copy()
df_scaled.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())

df_scaled.sample(5)


# Ahora, vamos a obtener registros similares para uno determinado, para cada combinaciÃ³n

# Probar con datos escalados y mÃ©trica euclidiana
result_scaled_euclidean = get_knn(df_scaled, n=0, k=5, metric="euclidean")
print(result_scaled_euclidean)

# Probar con datos escalados y mÃ©trica Manhattan
result_scaled_manhattan = get_knn(df_scaled, n=0, k=5, metric="manhattan")
print(result_scaled_manhattan)


# Respuestas a las preguntas

# **Â¿El hecho de que los datos no estÃ©n escalados afecta al algoritmo kNN? Si es asÃ­, Â¿cÃ³mo se manifiesta?** 
# 
# Respuesta: SÃ­, afecta. Cuando los datos no estÃ¡n escalados, las caracterÃ­sticas con rangos mÃ¡s amplios (como el income) dominan el cÃ¡lculo de las distancias, lo que puede llevar a resultados sesgados. El escalado normaliza las caracterÃ­sticas para que todas tengan el mismo peso en el cÃ¡lculo de la distancia.
# 

# **Â¿QuÃ© tan similares son los resultados al utilizar la mÃ©trica de distancia Manhattan (independientemente del escalado)?** 
# 
# Respuesta: La mÃ©trica Manhattan tiende a ser menos influenciada por valores extremos y da mÃ¡s importancia a las diferencias absolutas entre las caracterÃ­sticas, lo que puede llevar a vecinos ligeramente diferentes en comparaciÃ³n con la mÃ©trica Euclidiana.

# CÃ³digo para ValidaciÃ³n Adicional:
# Vamos a implementar este paso:

# Probar con diferentes clientes
print("Vecinos mÃ¡s cercanos para el cliente 10 con mÃ©trica Euclidiana:")
result_scaled_euclidean_10 = get_knn(df_scaled, n=10, k=5, metric="euclidean")
print(result_scaled_euclidean_10)

print("\nVecinos mÃ¡s cercanos para el cliente 20 con mÃ©trica Euclidiana:")
result_scaled_euclidean_20 = get_knn(df_scaled, n=20, k=5, metric="euclidean")
print(result_scaled_euclidean_20)

# Probar con diferentes valores de k
print("\nVecinos mÃ¡s cercanos para el cliente 0 con mÃ©trica Euclidiana, k=3:")
result_scaled_euclidean_k3 = get_knn(df_scaled, n=0, k=3, metric="euclidean")
print(result_scaled_euclidean_k3)

print("\nVecinos mÃ¡s cercanos para el cliente 0 con mÃ©trica Euclidiana, k=10:")
result_scaled_euclidean_k10 = get_knn(df_scaled, n=0, k=10, metric="euclidean")
print(result_scaled_euclidean_k10)


# ConclusiÃ³n de la ValidaciÃ³n:
# Consistencia: La funciÃ³n get_knn es consistente y devuelve vecinos similares al cliente objetivo, lo que confirma que estÃ¡ funcionando correctamente.
# VariaciÃ³n con k: Al variar k, la funciÃ³n sigue siendo robusta, aunque se observa que, al aumentar k, la variabilidad en las caracterÃ­sticas de los vecinos tambiÃ©n aumenta, lo cual es normal.

# # Tarea 2. Â¿Es probable que el cliente reciba una prestaciÃ³n del seguro?

# En tÃ©rminos de machine learning podemos considerarlo como una tarea de clasificaciÃ³n binaria.

# Con el valor de `insurance_benefits` superior a cero como objetivo, evalÃºa si el enfoque de clasificaciÃ³n kNN puede funcionar mejor que el modelo dummy.
# Instrucciones:
# - Construye un clasificador basado en KNN y mide su calidad con la mÃ©trica F1 para k=1...10 tanto para los datos originales como para los escalados. SerÃ­a interesante observar cÃ³mo k puede influir en la mÃ©trica de evaluaciÃ³n y si el escalado de los datos provoca alguna diferencia. Puedes utilizar una implementaciÃ³n ya existente del algoritmo de clasificaciÃ³n kNN de scikit-learn (consulta [el enlace](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) o tu propia implementaciÃ³n.- Construye un modelo dummy que, en este caso, es simplemente un modelo aleatorio. DeberÃ­a devolver "1" con cierta probabilidad. Probemos el modelo con cuatro valores de probabilidad: 0, la probabilidad de pagar cualquier prestaciÃ³n del seguro, 0.5, 1.
# La probabilidad de pagar cualquier prestaciÃ³n del seguro puede definirse como
# $$
# P\{\text{prestaciÃ³n de seguro recibida}\}=\frac{\text{nÃºmero de clientes que han recibido alguna prestaciÃ³n de seguro}}{\text{nÃºmero total de clientes}}.
# $$
# 
# Divide todos los datos correspondientes a las etapas de entrenamiento/prueba respetando la proporciÃ³n 70:30.

# Paso 1: Definir la Variable Objetivo

# Ñalcula el objetivo , definir la variable objetivo
df['insurance_benefits_received'] = (df['insurance_benefits'] > 0).astype(int) #<tu cÃ³digo aquÃ­>

# comprueba el desequilibrio de clases con value_counts()

print(df['insurance_benefits_received'].value_counts(normalize=True))


# Paso 2: Dividir los Datos

# Definir las caracterÃ­sticas y la variable objetivo
X = df[['gender', 'age', 'income', 'family_members']]  # CaracterÃ­sticas
y = df['insurance_benefits_received']  # Variable objetivo

# Dividir los datos en entrenamiento y prueba (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# Paso 3: Evaluar el Clasificador kNN
# AquÃ­ es donde entrenas el modelo kNN y evalÃºas su rendimiento. UtilizarÃ¡s el cÃ³digo proporcionado para construir un clasificador kNN y medir la mÃ©trica F1 para diferentes valores de k

def eval_classifier(y_true, y_pred):
    
    f1_score = sklearn.metrics.f1_score(y_true, y_pred)
    print(f'F1: {f1_score:.2f}')
    
# si tienes algÃºn problema con la siguiente lÃ­nea, reinicia el kernel y ejecuta el cuaderno de nuevo    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')
    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')
    print('Matriz de confusiÃ³n')
    print(cm)
    

# Entrenar y evaluar el clasificador kNN
for k in range(1, 11):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    print(f'k={k}')
    eval_classifier(y_test, y_pred)


# Paso 4: Construir el Modelo Dummy
# Finalmente, construimos el modelo dummy que predice de manera aleatoria si un cliente recibirÃ¡ beneficios. Este modelo sirve como referencia para comparar el rendimiento del kNN.

# generar la salida de un modelo aleatorio

def rnd_model_predict(P, size, seed=42):

    rng = np.random.default_rng(seed=seed)
    return rng.binomial(n=1, p=P, size=size)


# Calcular la probabilidad observada de recibir una prestaciÃ³n
P_observed = df['insurance_benefits_received'].sum() / len(df)


# Evaluar el modelo dummy con diferentes probabilidades
for P in [0, P_observed, 0.5, 1]:
    print(f'Probabilidad: {P:.2f}')
    y_pred_rnd = rnd_model_predict(P, size=len(y_test))
    eval_classifier(y_test, y_pred_rnd)
    print()


# # Tarea 3. RegresiÃ³n (con regresiÃ³n lineal)

# Con `insurance_benefits` como objetivo, evalÃºa cuÃ¡l serÃ­a la RECM de un modelo de regresiÃ³n lineal.

# Construye tu propia implementaciÃ³n de regresiÃ³n lineal. Para ello, recuerda cÃ³mo estÃ¡ formulada la soluciÃ³n de la tarea de regresiÃ³n lineal en tÃ©rminos de LA. Comprueba la RECM tanto para los datos originales como para los escalados. Â¿Puedes ver alguna diferencia en la RECM con respecto a estos dos casos?
# 
# Denotemos- $X$: matriz de caracterÃ­sticas; cada fila es un caso, cada columna es una caracterÃ­stica, la primera columna estÃ¡ formada por unidades- $y$ â€” objetivo (un vector)- $\hat{y}$ â€” objetivo estimado (un vector)- $w$ â€” vector de pesos
# La tarea de regresiÃ³n lineal en el lenguaje de las matrices puede formularse asÃ­:
# $$
# y = Xw
# $$
# 
# El objetivo de entrenamiento es entonces encontrar esa $w$ w que minimice la distancia L2 (ECM) entre $Xw$ y $y$:
# 
# $$
# \min_w d_2(Xw, y) \quad \text{or} \quad \min_w \text{MSE}(Xw, y)
# $$
# 
# Parece que hay una soluciÃ³n analÃ­tica para lo anteriormente expuesto:
# $$
# w = (X^T X)^{-1} X^T y
# $$
# 
# La fÃ³rmula anterior puede servir para encontrar los pesos $w$ y estos Ãºltimos pueden utilizarse para calcular los valores predichos
# $$
# \hat{y} = X_{val}w
# $$

# Divide todos los datos correspondientes a las etapas de entrenamiento/prueba respetando la proporciÃ³n 70:30. Utiliza la mÃ©trica RECM para evaluar el modelo.

# Paso 1: Definir la Clase MyLinearRegression
# El pre-cÃ³digo sugiere construir una implementaciÃ³n personalizada de un modelo de regresiÃ³n lineal. Este modelo se basarÃ¡ en la fÃ³rmula matemÃ¡tica para obtener los pesos ğ‘¤ y realizar predicciones.

class MyLinearRegression:
    
    def __init__(self):
        
        self.weights = None    # Inicializa los pesos como None
    
    def fit(self, X, y):
         # AÃ±adir una columna de unos a X para representar el tÃ©rmino independiente (bias)
        # aÃ±adir las unidades
        X2 = np.append(np.ones([len(X), 1]), X, axis=1)
         # Calcular los pesos utilizando la fÃ³rmula: w = (X^T X)^-1 X^T y
        self.weights = np.linalg.inv(X2.T @ X2) @ X2.T @ y

    def predict(self, X):
        # AÃ±adir una columna de unos a X para representar el tÃ©rmino independiente (bias)
        # aÃ±adir las unidades
        X2 = np.append(np.ones([len(X), 1]), X, axis=1)
        # Realizar la predicciÃ³n utilizando los pesos calculados
        y_pred =  X2 @ self.weights
        
        return y_pred


# Paso 2: Definir la FunciÃ³n de EvaluaciÃ³n eval_regressor
# Vamos a definir la funciÃ³n que evaluarÃ¡ el rendimiento del modelo utilizando la mÃ©trica RECM (RMSE) y el coeficiente de determinaciÃ³n R2.

def eval_regressor(y_true, y_pred):
    # Calcular el RMSE
    rmse = math.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))
    print(f'RMSE: {rmse:.2f}')
    # Calcular el R2
    r2_score = math.sqrt(sklearn.metrics.r2_score(y_true, y_pred))
    print(f'R2: {r2_score:.2f}')    


# Paso 3: Entrenar y Evaluar el Modelo de RegresiÃ³n Lineal
# Ahora, entrenaremos el modelo de regresiÃ³n lineal personalizado (MyLinearRegression) utilizando los datos de entrenamiento, y luego evaluaremos su rendimiento en los datos de prueba.


# Paso 3: Preparar los datos y entrenar el modelo
X = df[['age', 'gender', 'income', 'family_members']].to_numpy()
y = df['insurance_benefits'].to_numpy()

# Dividir los datos en entrenamiento y prueba (70:30)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)

# Crear una instancia del modelo
lr = MyLinearRegression()

# Entrenar el modelo en los datos de entrenamiento
lr.fit(X_train, y_train)
print(lr.weights)

# Realizar predicciones en los datos de prueba
y_test_pred = lr.predict(X_test)
# Evaluar el modelo en los datos de prueba
eval_regressor(y_test, y_test_pred)


# Paso 4: Evaluar el Modelo con Datos Escalados
# TambiÃ©n es Ãºtil escalar los datos para ver si esto afecta la precisiÃ³n del modelo. Vamos a utilizar el MaxAbsScaler como en las tareas anteriores y repetir el proceso.

# Escalar los datos
scaler = MaxAbsScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Crear una instancia del modelo
lr_scaled = MyLinearRegression()

# Entrenar el modelo en los datos de entrenamiento escalados
lr_scaled.fit(X_train_scaled, y_train)

# Realizar predicciones en los datos de prueba escalados
y_test_pred_scaled = lr_scaled.predict(X_test_scaled)

# Evaluar el modelo en los datos de prueba escalados
eval_regressor(y_test, y_test_pred_scaled)


# El modelo de regresiÃ³n lineal personalizado ha demostrado ser eficaz para predecir el nÃºmero de prestaciones de seguro que un nuevo cliente podrÃ­a recibir, con un buen nivel de precisiÃ³n (RMSE bajo y R2 aceptable). AdemÃ¡s, el hecho de que los resultados sean consistentes tanto para los datos originales como para los escalados sugiere que el modelo es robusto y bien ajustado.

# # Tarea 4. Ofuscar datos

# Lo mejor es ofuscar los datos multiplicando las caracterÃ­sticas numÃ©ricas (recuerda que se pueden ver como la matriz $X$) por una matriz invertible $P$. 
# 
# $$
# X' = X \times P
# $$
# 
# Trata de hacerlo y comprueba cÃ³mo quedarÃ¡n los valores de las caracterÃ­sticas despuÃ©s de la transformaciÃ³n. Por cierto, la propiedad de invertibilidad es importante aquÃ­, asÃ­ que asegÃºrate de que $P$ sea realmente invertible.
# 
# Puedes revisar la lecciÃ³n 'Matrices y operaciones matriciales -> MultiplicaciÃ³n de matrices' para recordar la regla de multiplicaciÃ³n de matrices y su implementaciÃ³n con NumPy.

# Paso 1: Ofuscar los Datos con una Matriz Invertible
# Primero, necesitamos crear una matriz aleatoria ğ‘ƒ y comprobar que sea invertible. Luego, multiplicaremos la matriz 
# ğ‘‹ (caracterÃ­sticas numÃ©ricas de los datos) por ğ‘ƒ para obtener los datos ofuscados X.
# 


# Seleccionar las columnas con informaciÃ³n personal
personal_info_column_list = ['gender', 'age', 'income', 'family_members']
df_pn = df[personal_info_column_list]

X = df_pn.to_numpy()

# Generar una matriz aleatoria $P$.


rng = np.random.default_rng(seed=42)
P = rng.random(size=(X.shape[1], X.shape[1]))


# Comprobar que la matriz P sea invertible


if np.linalg.det(P) != 0:
    print("La matriz P es invertible.")
else:
    raise ValueError("La matriz P no es invertible. Intente generar otra matriz.")

# Ofuscar los datos
X_obfuscated = X @ P

# Mostrar algunos ejemplos de los datos originales y ofuscados
print("Datos originales:\n", X[:5])
print("Datos ofuscados:\n", X_obfuscated[:5])


# Â¿Puedes adivinar la edad o los ingresos de los clientes despuÃ©s de la transformaciÃ³n?

# AquÃ­, el objetivo es demostrar que si conocemos la matriz de transformaciÃ³n ğ‘ƒ, podemos recuperar los datos originales aplicando la operaciÃ³n inversa. Esto se puede hacer invirtiendo la matriz ğ‘ƒ y multiplicando los datos transformados por la inversa de ğ‘ƒ.

# Â¿Puedes recuperar los datos originales de $X'$ si conoces $P$? Intenta comprobarlo a travÃ©s de los cÃ¡lculos moviendo $P$ del lado derecho de la fÃ³rmula anterior al izquierdo. En este caso las reglas de la multiplicaciÃ³n matricial son realmente Ãºtiles

# Paso 2: Intentar Recuperar los Datos Originales
# Para comprobar que la transformaciÃ³n es reversible, vamos a intentar recuperar los datos originales utilizando la matriz 
# ğ‘ƒâˆ’1


# Recuperar los datos originales
P_inv = np.linalg.inv(P)
X_recovered = X_obfuscated @ P_inv


# Muestra los tres casos para algunos clientes- Datos originales
# - El que estÃ¡ transformado- El que estÃ¡ invertido (recuperado)

# Mostrar algunos ejemplos de los datos recuperados
print("\nDatos recuperados:\n", X_recovered[:5])

# Comparar los datos recuperados con los originales
print("\nDatos originales:\n", X[:5])  # Muestra los primeros 5 registros originales

print("\nDatos transformados:\n", X_obfuscated[:5])  # Muestra los primeros 5 registros transformados

print("\nDatos recuperados:\n", X_recovered[:5])  # Muestra los primeros 5 registros recuperados


# Seguramente puedes ver que algunos valores no son exactamente iguales a los de los datos originales. Â¿CuÃ¡l podrÃ­a ser la razÃ³n de ello?

# En la prÃ¡ctica, la inversiÃ³n de matrices y la multiplicaciÃ³n de matrices pueden introducir pequeÃ±os errores debido a la naturaleza de los cÃ¡lculos en coma flotante. Esto significa que los valores recuperados podrÃ­an no ser exactamente iguales a los originales, aunque deberÃ­an ser muy cercanos.
# 
# Si los valores recuperados no son exactamente iguales, esto se debe a la acumulaciÃ³n de errores numÃ©ricos en la operaciÃ³n de inversiÃ³n y multiplicaciÃ³n de matrices.

# 4 Prueba de que la ofuscaciÃ³n de datos puede funcionar con regresiÃ³n lineal

# En este proyecto la tarea de regresiÃ³n se ha resuelto con la regresiÃ³n lineal. Tu siguiente tarea es demostrar _analytically_ que el mÃ©todo de ofuscaciÃ³n no afectarÃ¡ a la regresiÃ³n lineal en tÃ©rminos de valores predichos, es decir, que sus valores seguirÃ¡n siendo los mismos. Â¿Lo puedes creer? Pues no hace falta que lo creas, Â¡tienes que que demostrarlo!

# Entonces, los datos estÃ¡n ofuscados y ahora tenemos $X \times P$ en lugar de tener solo $X$. En consecuencia, hay otros pesos $w_P$ como
# $$
# w = (X^T X)^{-1} X^T y \quad \Rightarrow \quad w_P = [(XP)^T XP]^{-1} (XP)^T y
# $$
# 
# Â¿CÃ³mo se relacionarÃ­an $w$ y $w_P$ si simplificÃ¡ramos la fÃ³rmula de $w_P$ anterior? 
# 
# Â¿CuÃ¡les serÃ­an los valores predichos con $w_P$? 
# 
# Â¿QuÃ© significa esto para la calidad de la regresiÃ³n lineal si esta se mide mediante la RECM?
# Revisa el ApÃ©ndice B Propiedades de las matrices al final del cuaderno. Â¡AllÃ­ encontrarÃ¡s fÃ³rmulas muy Ãºtiles!
# 
# No es necesario escribir cÃ³digo en esta secciÃ³n, basta con una explicaciÃ³n analÃ­tica.

# **Respuesta**

# Â¿CÃ³mo se relacionarÃ­an  $ğ‘¤$ y  $ğ‘¤_ğ‘ƒ$ si simplificÃ¡ramos la fÃ³rmula de  $ğ‘¤_ğ‘ƒ$ anterior?
# 
# Los nuevos pesos $w_P$ estÃ¡n relacionados con los pesos originales $w$ a travÃ©s de la inversa de la matriz de transformaciÃ³n ğ‘ƒ.
# 
# En concreto:  $W_P$=$P^{âˆ’1}$ $w$
# 
# Â¿CuÃ¡les serÃ­an los valores predichos con  $ğ‘¤_ğ‘ƒ$?
# 
# Los valores predichos $\hat{y}p$ utilizando los datos ofuscados serÃ¡n exactamente los mismos que los valores predichos utilizando los datos originales $\hat{y}$ Esto demuestra que la ofuscaciÃ³n de datos no afecta la calidad de las predicciones en tÃ©rminos de la regresiÃ³n lineal.
# $\hat{y}p$= $XI_w$ = Xw = $\hat{y}$
#  
# Â¿QuÃ© significa esto para la calidad de la regresiÃ³n lineal si esta se mide mediante la RECM? Revisa el ApÃ©ndice B Propiedades de las matrices al final del cuaderno. Â¡AllÃ­ encontrarÃ¡s fÃ³rmulas muy Ãºtiles!
# 
# Dado que los valores predichos $\hat{y}p$ son idÃ©nticos a los valores predichos $\hat{y}$ cuando utilizamos la ofuscaciÃ³n, las mÃ©tricas de evaluaciÃ³n como el RMSE (Root Mean Squared Error) y el $ğ‘…^2$ no se verÃ¡n afectadas. Por lo tanto, la calidad del modelo de regresiÃ³n lineal permanecerÃ¡ inalterada.

# **Prueba analÃ­tica**

# Pregunta 1: Â¿CÃ³mo se relacionarÃ­an los pesos $ğ‘¤$ y $ğ‘¤ğ‘ƒ$ si simplificÃ¡ramos la fÃ³rmula de $ğ‘¤ğ‘ƒ$?
# 
# *Partimos de la fÃ³rmula general de los pesos en la regresiÃ³n lineal:
# 
# w = ($X^T$ X)$^{-1}$ $X^T$y
# 
# *Con la matriz de caracterÃ­sticas ofuscada $X_p$ = X x P los nuevos pesos $W_p$ se calculan como:
# 
# $w_P$ = [$(X_P)^T$ $X_P$]$^{-1}$ $(X_P)^T$ $y$  = [$P^T$ $X^T$ $XP$]$^{-1}$ $P^T$ $X^T$ $y$
# 
# *Aplicando las propiedades de las matrices, podemos simplificar esto:
# 
# $w_P$ = $P^{-1}$ ($X^T$ $X$)$^{-1}$ $X^T$ $y$ = $P^{-1}$ $w$
# 
# ConclusiÃ³n: Los nuevos pesos $w_p$ estÃ¡n relacionados con los pesos originales $w$ a travÃ©s de la matriz inversa de la transformaciÃ³n $ğ‘ƒ$
# 
# Pregunta 2: Â¿CuÃ¡les serÃ­an los valores predichos con $w_p$ ?
# 
# *Los valores predichos en la regresiÃ³n lineal se obtienen como:
# 
# $\hat{y}$ = Xw
# 
# *Para los datos ofuscados, se tiene:
# 
# $\hat{y}p$= $X_p$$w_p$= ( X x P ) ($P^{-1}$w)
# 
# *Simplificando:
# 
# $\hat{y}p$= $XI_w$ = Xw = $\hat{y}$
# 
# *ConclusiÃ³n: Los valores predichos con $w_p$ serÃ¡n idÃ©nticos a los valores predichos con ğ‘¤
# 
# Pregunta 3 : La ofuscaciÃ³n de datos mediante la multiplicaciÃ³n por una matriz invertible $ğ‘ƒ$ no afecta la calidad de la regresiÃ³n lineal, ni en tÃ©rminos de los valores predichos ni en tÃ©rminos de las mÃ©tricas de evaluaciÃ³n como RMSE o $R^2$
# 

# 5 Prueba de regresiÃ³n lineal con ofuscaciÃ³n de datos

# Ahora, probemos que la regresiÃ³n lineal pueda funcionar, en tÃ©rminos computacionales, con la transformaciÃ³n de ofuscaciÃ³n elegida.
# Construye un procedimiento o una clase que ejecute la regresiÃ³n lineal opcionalmente con la ofuscaciÃ³n. Puedes usar una implementaciÃ³n de regresiÃ³n lineal de scikit-learn o tu propia implementaciÃ³n.
# Ejecuta la regresiÃ³n lineal para los datos originales y los ofuscados, compara los valores predichos y los valores de las mÃ©tricas RMSE y $R^2$. Â¿Hay alguna diferencia?

# **Procedimiento**
# 
# - Crea una matriz cuadrada $P$ de nÃºmeros aleatorios.- Comprueba que sea invertible. Si no lo es, repite el primer paso hasta obtener una matriz invertible.- <Â¡ tu comentario aquÃ­ !>
# - Utiliza $XP$ como la nueva matriz de caracterÃ­sticas

# Paso 1: Crear una matriz cuadrada ğ‘ƒ de nÃºmeros aleatorios e invertirla.


# Ya se generÃ³ anteriormente una matriz P y se comprobÃ³ que es invertible.
# Continuamos utilizando esa misma matriz P y P_inv.


# Paso 2: Utilizar ğ‘‹Ã—ğ‘ƒ como la nueva matriz de caracterÃ­sticas y comparar los resultados.

# Crear el modelo de regresiÃ³n lineal utilizando los datos originales
model_original = LinearRegression()
model_original.fit(X_train, y_train)
y_pred_original = model_original.predict(X_test)

# RMSE y R2 para los datos originales
rmse_original = mean_squared_error(y_test, y_pred_original, squared=False)
r2_original = r2_score(y_test, y_pred_original)

# Crear el modelo de regresiÃ³n lineal utilizando los datos ofuscados
X_train_obfuscated = X_train @ P
X_test_obfuscated = X_test @ P

model_obfuscated = LinearRegression()
model_obfuscated.fit(X_train_obfuscated, y_train)
y_pred_obfuscated = model_obfuscated.predict(X_test_obfuscated)

# RMSE y R2 para los datos ofuscados
rmse_obfuscated = mean_squared_error(y_test, y_pred_obfuscated, squared=False)
r2_obfuscated = r2_score(y_test, y_pred_obfuscated)

# Mostrar los resultados
print(f"RMSE (original): {rmse_original:.4f}")
print(f"R2 (original): {r2_original:.4f}")
print(f"RMSE (ofuscado): {rmse_obfuscated:.4f}")
print(f"R2 (ofuscado): {r2_obfuscated:.4f}")


# # Conclusiones

# 1. Igualdad de Resultados Entre Datos Originales y Ofuscados:
# RMSE (RaÃ­z del Error CuadrÃ¡tico Medio): Tanto para los datos originales como para los datos ofuscados, la RMSE es exactamente la misma (0.3436). Esto significa que la precisiÃ³n del modelo, en tÃ©rminos de error medio, no se ve afectada por la ofuscaciÃ³n de los datos.
# RÂ² (Coeficiente de DeterminaciÃ³n): Al igual que la RMSE, el RÂ² es idÃ©ntico para ambos conjuntos de datos (0.4305). Esto indica que la capacidad del modelo para explicar la varianza en los datos objetivo es la misma, independientemente de si los datos han sido ofuscados o no.

# 2. Impacto de la OfuscaciÃ³n en la RegresiÃ³n Lineal:
# La ofuscaciÃ³n de datos mediante la multiplicaciÃ³n de las caracterÃ­sticas por una matriz invertible no altera los resultados de la regresiÃ³n lineal. Los valores predichos y las mÃ©tricas de evaluaciÃ³n (RMSE y RÂ²) permanecen inalterados.
# Esto se debe a que la multiplicaciÃ³n por una matriz invertible y su inversa es una transformaciÃ³n lineal que preserva la estructura de los datos en tÃ©rminos de relaciones lineales. La regresiÃ³n lineal, siendo una tÃ©cnica que busca relaciones lineales entre las caracterÃ­sticas y el objetivo, es robusta frente a este tipo de transformaciones.

# 3. Implicaciones PrÃ¡cticas:
# La ofuscaciÃ³n de datos es una tÃ©cnica viable para proteger la informaciÃ³n sensible sin comprometer la precisiÃ³n y eficacia de los modelos de machine learning, especÃ­ficamente en el contexto de la regresiÃ³n lineal.
# Esta propiedad es especialmente valiosa en aplicaciones donde la privacidad y la seguridad de los datos son crÃ­ticas, permitiendo que se mantenga la funcionalidad del modelo sin exponer informaciÃ³n confidencial.
# En resumen, la prueba demuestra que la ofuscaciÃ³n de datos, cuando se realiza correctamente, no afecta negativamente el rendimiento de la regresiÃ³n lineal, lo que la convierte en una tÃ©cnica efectiva para proteger los datos en contextos donde la privacidad es fundamental.






